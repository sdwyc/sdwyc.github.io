<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://sdwyc.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sdwyc.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-09T02:01:12+00:00</updated><id>https://sdwyc.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">混合整数规划与混合整数二次规划</title><link href="https://sdwyc.github.io/blog/2024/%E6%B7%B7%E5%90%88%E6%95%B4%E6%95%B0%E8%A7%84%E5%88%92%E4%B8%8E%E6%B7%B7%E5%90%88%E6%95%B4%E6%95%B0%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92/" rel="alternate" type="text/html" title="混合整数规划与混合整数二次规划"/><published>2024-07-26T19:03:00+00:00</published><updated>2024-07-26T19:03:00+00:00</updated><id>https://sdwyc.github.io/blog/2024/%E6%B7%B7%E5%90%88%E6%95%B4%E6%95%B0%E8%A7%84%E5%88%92%E4%B8%8E%E6%B7%B7%E5%90%88%E6%95%B4%E6%95%B0%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92</id><content type="html" xml:base="https://sdwyc.github.io/blog/2024/%E6%B7%B7%E5%90%88%E6%95%B4%E6%95%B0%E8%A7%84%E5%88%92%E4%B8%8E%E6%B7%B7%E5%90%88%E6%95%B4%E6%95%B0%E4%BA%8C%E6%AC%A1%E8%A7%84%E5%88%92/"><![CDATA[<h1 id="引言">引言</h1> <p>在数值优化领域中，混合整数规划（Mixed Integer Programming, MIP）和混合整数二次规划（Mixed Integer Quadratic Programming, MIQP）是两类重要的优化模型。本文将对这两种规划的基本概念、常用求解方法进行介绍，并通过实例分析帮助大家更好地理解这些规划问题。</p> <h1 id="一混合整数规划mip或milp基本概念">一、混合整数规划（MIP或MILP）基本概念</h1> <p>混合整数规划问题是指在优化过程中，部分优化变量必须取整数值，其他变量则可以取连续值的一类优化问题。</p> <p>典型的MIP问题形式如下：</p> <table> <thead> <tr> <th>目标方程：</th> <th>\(\min \ \mathbf{c} ^T \mathbf{x}\)</th> </tr> </thead> <tbody> <tr> <td>约束：</td> <td>\(\mathbf{A} \mathbf{x} \le\mathbf{b}\) (线性约束)</td> </tr> <tr> <td> </td> <td>\(\mathbf{x} _l\le \mathbf{x} \le \mathbf{x} _h\)(边界约束)</td> </tr> <tr> <td> </td> <td>\(\mathbf{x}_i​∈Z\), \(\mathbf{x}_j​∈R\) (for some i, others j) （整数约束）</td> </tr> </tbody> </table> <p>这里，\(\mathbf{c}\)和\(\mathbf{x}\) 是向量，\(\mathbf{A}\)是矩阵，\(\mathbf{b}\)是约束条件。换句话说，混合整数规划的目标函数与线性规划不同的是，优化变量的定义域可能是整数，这就给求解带来了极大的难度。</p> <h1 id="二混合整数二次规划基本概念">二、混合整数二次规划基本概念</h1> <p>混合整数二次规划是混合整数规划的扩展，其目标函数是一个二次函数，而约束条件通常为一次形式。</p> <p>典型的MIQP问题形式如下：</p> <table> <thead> <tr> <th>目标方程：</th> <th>\(\min \ \mathbf{x}^{T}\mathbf{Q}\mathbf{x}+ \mathbf{c} ^T \mathbf{x}\)</th> </tr> </thead> <tbody> <tr> <td>约束：</td> <td>\(\mathbf{A} \mathbf{x} \le\mathbf{b}\) (线性约束)</td> </tr> <tr> <td> </td> <td>\(\mathbf{x} _l\le \mathbf{x} \le \mathbf{x} _h\)(边界约束)</td> </tr> <tr> <td> </td> <td>\(\mathbf{x}_i​∈Z\), \(\mathbf{x}_j​∈R\) (for some i, others j) （整数约束）</td> </tr> </tbody> </table> <p>上述变量定义与MIP完全相同，可以看到MIQP其实就是再MIP的目标函数的基础上，加上了一个二次项。</p> <h1 id="三常见求解方法">三、常见求解方法</h1> <h2 id="1-分支定界法bnb最常用">1. 分支定界法BnB（最常用）</h2> <p>分支定界法是求解MIP和MIQP问题的经典方法之一。其基本思想是将问题分解为一系列子问题，通过构建一个分支树来逐步逼近最优解。分支定界法主要包括三个步骤：分支、定界和剪枝。简单说一下基于LP的BnB方法：</p> <p>首先，对初始的MILP删除所有整数约束，得到原MILP的线性规划松弛。接下来，求解这个线性规划松弛（LP）。如果得到的解恰好满足所有整数约束，那么这个解就是原始MILP的最优解，运算终止。如果解没有满足所有整数约束（这种情况较为常见），则需要选择某个整数约束实际上得到小数值的变量进行“分支”。为了便于说明，假设这个变量是x，其在LP解中的值是5.7。我们可以通过施加\(x≤5.0\)和\(x≥6.0\)的限制来排除该值5.7。</p> <p>如果用\(P\)表示原MILP问题，用\(P_1\)表示新的MILP（增加了\(x≤5.0\)的约束），用\(P_2\)表示新的MILP（增加了\(x≥6.0\)的约束）。变量\(x\)被称为分支变量，我们在\(x\)上进行了分支，产生了两个子MILP \(P_1\)和 \(P_2\)。显然，如果我们能够求解$P_1$和$P_2$的最优解，那么其中较好的解就是原始问题\(P\)的最优解。</p> <p>通过这种方式，我们用两个更简单（更少整数约束）的MILP取代了\(P\)。我们现在将相同的思想应用于这两个子MILP，并在必要时选择新的分支变量。这样生成了所谓的搜索树。搜索过程生成的MILP称为树的节点，\(P\)为根节点。叶子节点是尚未分支的所有节点。如果所有叶节点的解都满足原MILP的整数约束，那么原始MILP问题就得到了求解。</p> <h2 id="2-割平面法cutting-plane">2. 割平面法（Cutting Plane）</h2> <p>割平面法通过添加额外的约束条件（即割平面）来缩小可行解空间，从而更快地找到最优解。对于MIP和MIQP问题，可以通过不断添加有效的割平面来逼近整数解。</p> <p>割平面通常认为是MIP中提升计算效率的最重要技巧。其基本思想为：思想是通过去除一些非整数解，就想MIP-based presolve一样，达到tighten formulation的目的。割平面不像branching，branching会产生两个子问题(两个分支)，cutting plane只是切掉一些边角(如下图)，不会产生2个MIP。</p> <h2 id="3-启发式算法heuristic">3. 启发式算法（Heuristic）</h2> <p>启发式算法是一类通过经验和启发信息来快速找到满意解的方法。常见的启发式算法包括遗传算法、模拟退火算法和粒子群优化算法。这些方法通常不能保证找到全局最优解，但可以在较短时间内找到质量较好的解。</p> <h2 id="4-混合算法hybrid-algorithms">4. 混合算法（Hybrid Algorithms）</h2> <p>混合算法结合了多种求解方法的优点，以期提高求解效率和解的质量。例如，可以将启发式算法与分支定界法结合，利用启发式算法快速找到初始解，然后通过分支定界法进行精细优化等等。</p> <h1 id="四实例代码求解">四、实例代码求解</h1> <p>本节我们举一个简单的生产实例，来说明如何使用Python的<strong>Pulp</strong>库进行MIP和MIQP问题求解，问题描述如下:</p> <h2 id="1-生产计划问题milp问题"><strong>1. 生产计划问题（MILP问题）</strong></h2> <p>假设某工厂生产两种产品产品\(a\)和产品\(b\)，每种产品都有其生产成本和销售利润，其中：</p> <ul> <li>产品\(a\)的生产成本为10元，销售价格为15元。</li> <li>产品\(b\)的生产成本为20元，销售价格为30元。</li> </ul> <p>该工厂的生产能力和原材料有限，假设：</p> <ul> <li>每月生产能力为70单位。</li> <li>每月原材料限制为150单位，生产产品\(a\)消耗1单位原材料，生产产品\(b\)消耗3单位原材料。</li> </ul> <p>问题一： 如何决定每种产品数量，以实现最大化利润？</p> <ul> <li> <p>问题分析：</p> <p>从问题中，我们可以看出，待优化的决策变量为产品\(a\)和产品\(b\)的数量，用\(x_1\)和\(x_2\)表示，其中\(x_1\)和\(x_2\)必须取整数值，这个问题可以建模为一个MIP或MILP问题。</p> <p>其中，我们的目标函数为：</p> \[\max {(15-10)x_1 + (30-20)x_2}\] <p>约束为：</p> \[\begin{array}{c} x_1+x_2 \le 70 \\ x_1+3x_2 \le 150 \\ x_1, x_2 \in Z^{+} \end{array}\] </li> </ul> <h3 id="python求解代码基于pulp库">Python求解代码（基于Pulp库）</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pulp</span> <span class="k">as</span> <span class="n">pl</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">solver_main</span><span class="p">():</span>

    <span class="n">ProbLp</span><span class="o">=</span><span class="n">pl</span><span class="p">.</span><span class="nc">LpProblem</span><span class="p">(</span><span class="sh">"</span><span class="s">ProbLp</span><span class="sh">"</span><span class="p">,</span><span class="n">sense</span><span class="o">=</span><span class="n">pl</span><span class="p">.</span><span class="n">LpMaximize</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">ProbLp</span><span class="p">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">x1</span><span class="o">=</span><span class="n">pl</span><span class="p">.</span><span class="nc">LpVariable</span><span class="p">(</span><span class="sh">'</span><span class="s">x_1</span><span class="sh">'</span><span class="p">,</span><span class="n">lowBound</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">upBound</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">cat</span><span class="o">=</span><span class="sh">'</span><span class="s">Integer</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">x2</span><span class="o">=</span><span class="n">pl</span><span class="p">.</span><span class="nc">LpVariable</span><span class="p">(</span><span class="sh">'</span><span class="s">x_2</span><span class="sh">'</span><span class="p">,</span><span class="n">lowBound</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">upBound</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">cat</span><span class="o">=</span><span class="sh">'</span><span class="s">Integer</span><span class="sh">'</span><span class="p">)</span>
 
    <span class="n">ProbLp</span><span class="o">+=</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="n">x1</span><span class="o">+</span><span class="mi">10</span><span class="o">*</span><span class="n">x2</span><span class="p">)</span> <span class="c1"># Add Objective
</span>    <span class="c1"># Add Constraints
</span>    <span class="n">ProbLp</span><span class="o">+=</span><span class="p">(</span><span class="n">x1</span><span class="o">+</span><span class="n">x2</span><span class="o">&lt;=</span><span class="mi">70</span><span class="p">)</span> 
    <span class="n">ProbLp</span><span class="o">+=</span><span class="p">(</span><span class="n">x1</span><span class="o">+</span><span class="mi">3</span><span class="o">*</span><span class="n">x2</span><span class="o">&lt;=</span><span class="mi">150</span><span class="p">)</span>
    <span class="n">ProbLp</span><span class="p">.</span><span class="nf">solve</span><span class="p">()</span>
    <span class="c1"># Log for solver
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Shan Status:</span><span class="sh">"</span><span class="p">,</span> <span class="n">pl</span><span class="p">.</span><span class="n">LpStatus</span><span class="p">[</span><span class="n">ProbLp</span><span class="p">.</span><span class="n">status</span><span class="p">])</span> 
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Optimal objective value</span><span class="sh">"</span><span class="p">,</span> <span class="n">pl</span><span class="p">.</span><span class="nf">value</span><span class="p">(</span><span class="n">ProbLp</span><span class="p">.</span><span class="n">objective</span><span class="p">))</span> 
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">ProbLp</span><span class="p">.</span><span class="nf">variables</span><span class="p">():</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="p">,</span> <span class="n">v</span><span class="p">.</span><span class="n">varValue</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="nf">solver_main</span><span class="p">()</span>
</code></pre></div></div> <p>执行脚本，求得最优解：\(x_1=30，x_2=40\)，输出如下:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Welcome to the CBC MILP Solver
Version: 2.10.3
Build Date: Dec 15 2019

<span class="nb">command </span>line - D:<span class="se">\A</span>naconda3<span class="se">\e</span>nvs<span class="se">\t</span>orch_env<span class="se">\l</span>ib<span class="se">\s</span>ite-packages<span class="se">\p</span>ulp<span class="se">\s</span>olverdir<span class="se">\c</span>bc<span class="se">\w</span><span class="k">in</span><span class="se">\6</span>4<span class="se">\c</span>bc.exe C:<span class="se">\U</span>sers<span class="se">\A</span>DMINI~1<span class="se">\A</span>ppData<span class="se">\L</span>ocal<span class="se">\T</span>emp<span class="se">\7</span>a689790a63c4d0bbd42e787d4d055db-pulp.mps <span class="nt">-max</span> <span class="nt">-timeMode</span> elapsed <span class="nt">-branch</span> <span class="nt">-printingOptions</span> all <span class="nt">-solution</span> C:<span class="se">\U</span>sers<span class="se">\A</span>DMINI~1<span class="se">\A</span>ppData<span class="se">\L</span>ocal<span class="se">\T</span>emp<span class="se">\7</span>a689790a63c4d0bbd42e787d4d055db-pulp.sol 
<span class="o">(</span>default strategy 1<span class="o">)</span>
At line 2 NAME          MODEL
At line 3 ROWS
At line 7 COLUMNS
At line 18 RHS
At line 21 BOUNDS
At line 24 ENDATA
Problem MODEL has 2 rows, 2 columns and 4 elements
Coin0008I MODEL <span class="nb">read </span>with 0 errors
Option <span class="k">for </span>timeMode changed from cpu to elapsed
Continuous objective value is 550 - 0.00 seconds
Cgl0004I processed model has 2 rows, 2 columns <span class="o">(</span>2 integer <span class="o">(</span>0 of which binary<span class="o">))</span> and 4 elements
Cutoff increment increased from 1e-05 to 4.9999
Cbc0012I Integer solution of <span class="nt">-550</span> found by DiveCoefficient after 0 iterations and 0 nodes <span class="o">(</span>0.00 seconds<span class="o">)</span> 
Cbc0001I Search completed - best objective <span class="nt">-550</span>, took 0 iterations and 0 nodes <span class="o">(</span>0.00 seconds<span class="o">)</span>
Cbc0035I Maximum depth 0, 0 variables fixed on reduced cost
Cuts at root node changed objective from <span class="nt">-550</span> to <span class="nt">-550</span>
Probing was tried 0 <span class="nb">times </span>and created 0 cuts of which 0 were active after adding rounds of cuts <span class="o">(</span>0.000 seconds<span class="o">)</span>
Gomory was tried 0 <span class="nb">times </span>and created 0 cuts of which 0 were active after adding rounds of cuts <span class="o">(</span>0.000 seconds<span class="o">)</span>
Knapsack was tried 0 <span class="nb">times </span>and created 0 cuts of which 0 were active after adding rounds of cuts <span class="o">(</span>0.000 seconds<span class="o">)</span>
Clique was tried 0 <span class="nb">times </span>and created 0 cuts of which 0 were active after adding rounds of cuts <span class="o">(</span>0.000 seconds<span class="o">)</span>
MixedIntegerRounding2 was tried 0 <span class="nb">times </span>and created 0 cuts of which 0 were active after adding rounds of 
cuts <span class="o">(</span>0.000 seconds<span class="o">)</span>
FlowCover was tried 0 <span class="nb">times </span>and created 0 cuts of which 0 were active after adding rounds of cuts <span class="o">(</span>0.000 
seconds<span class="o">)</span>
TwoMirCuts was tried 0 <span class="nb">times </span>and created 0 cuts of which 0 were active after adding rounds of cuts <span class="o">(</span>0.000 seconds<span class="o">)</span>
ZeroHalf was tried 0 <span class="nb">times </span>and created 0 cuts of which 0 were active after adding rounds of cuts <span class="o">(</span>0.000 seconds<span class="o">)</span>

Result - Optimal solution found

Objective value:                550.00000000
Enumerated nodes:               0
Total iterations:               0
Time <span class="o">(</span>CPU seconds<span class="o">)</span>:             0.00
Time <span class="o">(</span>Wallclock seconds<span class="o">)</span>:       0.00

Option <span class="k">for </span>printingOptions changed from normal to all
Total <span class="nb">time</span> <span class="o">(</span>CPU seconds<span class="o">)</span>:       0.01   <span class="o">(</span>Wallclock seconds<span class="o">)</span>:       0.01

Shan Status: Optimal
Optimal objective value 550.0
x_1 <span class="o">=</span> 30.0
x_2 <span class="o">=</span> 40.0
</code></pre></div></div> <h2 id="2-考虑收益衰减的生产问题miqp问题">2. 考虑收益衰减的生产问题（MIQP问题）</h2> <p>在示例1中的生产问题是一种基本的生产模型，在实际生产过程中，产品的利润有可能会随着产品的数量增加而逐渐降低，呈现收益衰减的现象。我们在示例1的基础上，不妨假设：</p> <ul> <li>产品\(a\)的生产成本为10元，销售价格为\(15-0.03{x_1}\)。</li> <li>产品\(b\)的生产成本为20元，销售价格为\(30-0.02{x_2}\)元。</li> </ul> <p>工厂生产能力和原材料与示例1相同，假设：</p> <ul> <li>每月生产能力为70单位。</li> <li>每月原材料限制为150单位，生产产品\(a\)消耗1单位原材料，生产产品\(b\)消耗3单位原材料。</li> </ul> <p>问题二： 如何决定每种产品数量，以实现最大化利润？</p> <ul> <li> <p>问题分析：</p> <p>从问题中，待优化的决策变量依然为产品\(a\)和产品\(b\)的数量，用\(x_1\)和\(x_2\)表示，其中\(x_1\)和\(x_2\)必须取整数值，但是收益模型变得更加复杂了。</p> <p>其中，我们的目标函数为：</p> \[\begin{array}{c} \max {(15-0.03x_1-10)x_1+(30-0.02x_2-20)x_2} \\ = \max {(-0.03{x_1}^{2}-0.02{x_2}^{2}+5x_1+10x_2)} \end{array}\] <p>约束不变，仍为：</p> \[\begin{array}{c} x_1+x_2 \le 70 \\ x_1+3x_2 \le 150 \\ x_1, x_2 \in Z^{+} \end{array}\] <p>我们可以看出优化的目标函数为二次形式，因此此任务可以被建模为一个MIQP问题，我们将其转换为矩阵的表示形式：</p> \[\begin{array}{c} \max {\mathbf{x} ^{T}\begin{bmatrix} -0.03 &amp; \\ &amp; -0.02 \end{bmatrix}\mathbf{x} + \begin{bmatrix} 5\\ 10 \end{bmatrix}^{T}\mathbf{x} } \\ s.t.\ \begin{bmatrix} 1 &amp; 1\\ 1 &amp; 3 \end{bmatrix}\mathbf{x} \le \begin{bmatrix} 70\\ 150 \end{bmatrix}, x_1, x_2 \in Z^{+} \end{array}\] </li> </ul> <h2 id="python求解代码基于scip-solvercvxpy">Python求解代码（基于SCIP solver+cvxpy）</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">cvxpy</span> <span class="k">as</span> <span class="n">cp</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">solver_main</span><span class="p">():</span>
    <span class="c1"># Coefficient setting
</span>    <span class="n">mat_Q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">diag</span><span class="p">([</span><span class="o">-</span><span class="mf">0.03</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02</span><span class="p">])</span>
    <span class="c1"># print('Q= ', mat_Q)
</span>    <span class="n">mat_c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
    <span class="c1"># print('c= ', mat_c)
</span>    <span class="n">mat_A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">mat_A</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">mat_b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">70</span><span class="p">,</span> <span class="mi">150</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">cp</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">integer</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># Optimization setting
</span>    <span class="n">objective</span> <span class="o">=</span> <span class="n">cp</span><span class="p">.</span><span class="nc">Maximize</span><span class="p">(</span><span class="n">cp</span><span class="p">.</span><span class="nc">QuadForm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mat_Q</span><span class="p">)</span> <span class="o">+</span> <span class="n">mat_c</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">constraints</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mat_A</span> <span class="o">@</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="n">mat_b</span><span class="p">]</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="n">cp</span><span class="p">.</span><span class="nc">Problem</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">constraints</span><span class="p">)</span>
    <span class="n">prob</span><span class="p">.</span><span class="nf">solve</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">The Optimal objective value: </span><span class="sh">'</span><span class="p">,</span> <span class="n">prob</span><span class="p">.</span><span class="n">value</span><span class="p">)</span>   <span class="c1"># Optimal value
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">[x_1, x_2]: </span><span class="sh">'</span> <span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">value</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="nf">solver_main</span><span class="p">()</span>
</code></pre></div></div> <p>运行脚本，你将看到优化后的结果为：</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The Optimal objective value:  491.0000000181662
<span class="o">[</span>x_1, x_2]:  <span class="o">[</span>30.00000001 40.        <span class="o">]</span>
</code></pre></div></div> <blockquote> <p>注意：复杂的MIP和MIQP问题在数学领域中是极难求解的，一般求解器求得的都是近似最优解（取决于求解器的求解方法），求解整数规划的求解器主要有：<strong>Gurobi</strong>，<strong>SCIP</strong>，<strong>CBC，OSQP</strong>等等。</p> </blockquote> <p>参考链接：</p> <p><a href="https://www.gurobi.com/resources/mixed-integer-programming-mip-a-primer-on-the-basics/">Gurobi base resource</a></p> <p><a href="https://www.cvxpy.org/examples/basic/quadratic_program.html">cvxpy tutorial</a></p> <p><a href="https://blog.csdn.net/weixin_44077955/article/details/126607480">混合整数规划（Mixed Integer Programming）</a></p> <p><a href="https://blog.csdn.net/weixin_46039719/article/details/121695205">整数规划（Python)</a></p>]]></content><author><name></name></author><category term="math"/><category term="Optimization"/><summary type="html"><![CDATA[MIP, MIQP介绍和实例应用]]></summary></entry><entry><title type="html">以SLAMer角度看待GN和LM算法</title><link href="https://sdwyc.github.io/blog/2024/%E4%BB%A5SLAMer%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%85GN%E5%92%8CLM%E7%AE%97%E6%B3%95/" rel="alternate" type="text/html" title="以SLAMer角度看待GN和LM算法"/><published>2024-06-20T16:03:00+00:00</published><updated>2024-06-20T16:03:00+00:00</updated><id>https://sdwyc.github.io/blog/2024/%E4%BB%A5SLAMer%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%85GN%E5%92%8CLM%E7%AE%97%E6%B3%95</id><content type="html" xml:base="https://sdwyc.github.io/blog/2024/%E4%BB%A5SLAMer%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%85GN%E5%92%8CLM%E7%AE%97%E6%B3%95/"><![CDATA[<h1 id="引言">引言</h1> <p>SLAM作为机器人和自动驾驶领域的核心问题之一，其目标是在未知环境中实现同时的定位和地图构建。其中在求解多观测约束下的位姿估计问题时，GN和LM算法在求解非线性最小二乘问题方面扮演了重要的角色。为嘛呢？因为这俩思想非常简单实用。博主个人认为，这两种方法的一个共同的优点是快，简单，准确度可以接受。接下来，从一个SLAMer的角度看待GN和LM算法。<strong>注意</strong>，本篇不涉及代码编程和复杂的理论说明，仅有公式推导，旨在用白话来讨论GN和LM算法。如有逻辑上的不严谨，欢迎各位批评指证。</p> <hr/> <h1 id="1-一个非线性优化问题的建模">1. 一个非线性优化问题的建模</h1> <p>一个SLAM问题通常可以被描述为一个最大后验概率问题，也叫MAP（Maximum ）问题， 即我们想根据历史时刻<strong>已观测到的数据\(z\)和状态\(x\)，以及控制量\(u\)</strong>，得到最有可能的下一时刻的状态\(x_{t+1}\)（因为SLAM的核心本质是Localization嘛），用公式来描述就是</p> \[x_{t+1} = \underset{x}{argmax} P(x_{t+1}|z_{0:t}, x_{0:t},u_{0:t})\] <p>这其实是一个很抽象的描述公式，因为我们能通过公式看到SLAM问题的目的是什么，但是我们无法找到一个具体的代入方法和求解方法。并且，求解MAP问题本身是一个比较复杂的问题，求解方法也有很多种，比如 粒子滤波，卡尔曼滤波等等。而本章所说的非线性优化，也是求解MAP问题的一个有效的途径。</p> <p>我们以一个纯激光SLAM为例，激光雷达带来的是一系列的点云，我们可以把<strong>每一个点</strong>都看作是一次对环境的观测，对一个SLAM（或者是Localization）问题，最重要的是要<strong>推算出雷达的当前位置\(\mathbf{x}^{L}_{t}\)</strong>，而且我们还知道在一个<strong>3维空间下的刚性变换\(\mathbf{T}\)可以用旋转矩阵\(\mathbf{R}\)和平移向量\(\mathbf{t}\)来表示，</strong>那么我们可以将雷达的当前位置表示为</p> \[\mathbf{x}^{L}_{t} = \prod_{i=0}^{t-1} \mathbf{\bigtriangleup T}_{i} \cdot \mathbf{x}^{L}_{0}\] <p>用大白话讲，雷达当前的位置相当于对雷达初始位置\(\mathbf{x}^{L}_{0}\)逐步进行变换\(\mathbf{\bigtriangleup T}_{i}\)即可，那么我们的问题就转换为如何求取\(\mathbf{\bigtriangleup T}_{i}\)。</p> <p>现在我们假设，经过一个刚性变换$\mathbf{T}$的前后两帧雷达点云分别为\(P_{A}=\{ p^{A}_{i} \}\)和\(\mathcal{P}_{B}=\{ \mathbf{p}_{j} \}\)，并我们在不考虑噪声和异常点（outlier）的前提下，这两坨点云理应满足</p> \[\begin{matrix} \mathcal{P}_{B}= \mathbf{T}\mathcal{P}_{A} \\\\ \sum ||\mathbf{p}^{B}_{j} - \mathbf{T} \mathbf{p}^{A}_{i}||_{2} = 0\end{matrix}\] <p>这两个式子意思就是，变换前的雷达帧历经一次刚性变换\(\mathbf{T}\)后，应该与变换后的点云完全重合（在点的对应关系已知的情况下）。But！这只是一个非常理想的情况，现实情境下，是不可以忽略噪声和异常点的！</p> <p>OK，那么我们现在将这些因素考虑进去。假设我们给每个点加一个高斯白噪声，即\(\mathbf{p}_{i}\sim N(\mathbf{p}_{i}, \mathbf{\Sigma}_{i})\)， 这里由于一个点可做是一个3维向量，因此每个点服从一个多维高斯分布。也就是每个点的位置分布情况是涉及到一个概率的，我们知道高斯分布有个优雅的性质，<strong>多个独立的高斯分布的线性组合依旧是高斯分布</strong>，那么我们可以推出</p> \[(\mathbf{p}^{B}_{j} - \mathbf{T} \mathbf{p}^{A}_{i}) \sim N(\mathbf{d}_{ij}, \mathbf{\Sigma}_{j} + \mathbf{T}^{T} \mathbf{\Sigma}_{i} \mathbf{T})\] <p>哎！？这个好，这个公式看起来很优雅，这时我们就在想，如果每个点之间都是独立的，这个\(**\mathbf{T}\)只要能让每个点对的差值\(\mathbf{d}_{ij}\)为0的联合概率最大**不就完了么！！吆西，那么说做就做，我们得到下面这个式子</p> \[\begin{align} \mathbf{T} &amp; = \underset{\mathbf{T} }{argmax} \prod P(\mathbf{p}^{B}_{j} - \mathbf{T} \mathbf{p}^{A}_{i})\\ &amp; \overset{log}{=} \underset{\mathbf{T} }{argmin} \sum \mathbf{d}_{ij}^{T}(\mathbf{\Sigma}_{j} + \mathbf{T}^{T} \mathbf{\Sigma}_{i} \mathbf{T})^{-1} \mathbf{d}_{ij}\\ &amp; = {\color{Red} \underset{\mathbf{T} }{argmin} \sum \mathbf{d}_{ij}^{T}(\mathbf{\Sigma}_{ij})^{-1} \mathbf{d}_{ij}} \end{align}\] <p>解释一下，公式（1）表示的是联合概率最大化，但是求积操作不好算，况且高斯分布里含有指数\(exp(\cdot)\)，所以在这里两边取对数操作来去掉指数和常数项，化积为和。<strong>最终化简出来的公式（3）正式描述了一个非线性最小二乘问题，也是ICP配准问题的核心公式</strong>。</p> <p>现在我们换一个思路来理解上面公式（3）并且将里面的符号转换成一个易于理解的形式。众所周知，最小二乘问题的本质思想是：<strong>让真实值与理论值的差距之和（即残差）最小</strong>。从这个思路出发，我们可以发现，公式（3）中的\(\mathbf{d}_{ij}^{T}\mathbf{d}_{ij}\)其实就是最小二乘中的<strong>残差项</strong>，而\((\mathbf{\Sigma}_{ij})^{-1}\)只不过是给每一个残差项成了一个权重系数，因此博主习惯把\((\mathbf{\Sigma}_{ij})^{-1}\)称为<strong>权重矩阵</strong>（当然这个矩阵的名号很多，比如说信息矩阵，马氏距离…）。这个如果点对的协方差越小，代表对位置的信任程度，就越高，自然权重就越大。OK，这样一个针对非线性优化的最小二乘问题就描述完了！ 下面呢，为了便于理解，我们还是<strong>用\(\mathbf{e}_{ij}\)表示残差，用\(\mathbf{\Omega}_{ij}\)表示权重矩阵（信息矩阵）</strong>，即</p> \[\mathbf{T} = \underset{\mathbf{T} }{argmin} \sum \mathbf{e}_{ij}^{T}\mathbf{\Omega }_{ij} \mathbf{e}_{ij} \quad\ \ \ (LSQ)\] <h1 id="2-高斯牛顿优化算法gauss-newton">2. 高斯牛顿优化算法（Gauss-Newton）</h1> <p>我们现在有要解决的目标优化问题，那么该如何求解呢，梯度下降法教会了我们通过迭代求解最小值时，要往梯度（导数）下降最大的方向走，但是如何求梯度呢？这个时候高斯牛顿法对此有了新的方案。</p> <p>话不多说，我们开始进入正题（原谅博主废话过多），第1章中每一个残差项可以看作是一个关于\(\mathbf{p}^{A}\)的函数（因为我们是想让通过调整\(\mathbf{P}_{A}\)来对齐\(\mathbf{P}_{B}\)），以下简称\(\mathbf{e}(\mathbf{p})\)。现在我们对\(\mathbf{e}(\mathbf{p})\)在\(\mathbf{p}\)附近进行一阶泰勒展开。</p> \[\mathbf{e}(\mathbf{p}+\Delta\mathbf{p}) = \mathbf{e}(\mathbf{p}) + \mathbf{J} (\mathbf{p})\Delta\mathbf{p} \quad \ \ \ \ (Taylor)\] <p>这里的\(\mathbf{J} (\mathbf{p})=\frac{d \mathbf{e} (\mathbf{p} )}{d\mathbf{p} }\) ，也就是江湖上令人闻风丧胆的雅可比矩阵（Jacobian Matrix），为啥闻风丧胆？且看后续推导，现在我们只需要假设已知\(\mathbf{J} (\mathbf{p})\)就好啦。</p> <blockquote class="block-tip"> <h5 id="这里需要说明两点">这里需要说明两点：</h5> <ol> <li>泰勒的意义是什么，在G-N优化中，我们并不是直接求两坨点云之间的变换\(\mathbf{T}\)，而是通过迭代求解，每次迭代会寻找一个的增量\(\Delta\mathbf{T}\)，使得\(\sum \mathbf{e}_{k}^{T}(\Delta\mathbf{T}\mathbf{p}_{k})\mathbf{\Omega }_{k} \mathbf{e}_{k}(\Delta\mathbf{T}\mathbf{p}_{k})\)尽可能的小</li> <li>符号的说明，有人就要怼我了，怎么还一会用\(\Delta\mathbf{T}\mathbf{p}\)，一会用\(\mathbf{p}+\Delta\mathbf{p}\)。其实这里两个表示方法是同一个意思，都表示为对一个点做一次刚性变换，只不过在推导数学公式时，我们可能更习惯使用求和而已。</li> </ol> </blockquote> <p>这里我们旨在求解每次迭代过程中的最优\(\Delta \mathbf{p}\)即可， 我们将上面泰勒展开式子代入到公式（LSQ）中，并进行推导</p> \[\begin{align} \bigtriangleup \mathbf{p} &amp; = \underset{\mathbf{\bigtriangleup p} }{argmin} \sum \mathbf{e}_{k}^{T}(\mathbf{p} +\mathbf{\bigtriangleup p} )\mathbf{\Omega }_{k} \mathbf{e}_{k}(\mathbf{p} +\mathbf{\bigtriangleup p}) \\ &amp; = \underset{\mathbf{\bigtriangleup p} }{argmin} \sum [\mathbf{e}_{k}(\mathbf{p}) + \mathbf{J}_{k} (\mathbf{p})\Delta\mathbf{p}]^{T} \mathbf{\Omega }_{k} [\mathbf{e}_{k}(\mathbf{p}) + \mathbf{J}_{k} (\mathbf{p})\Delta\mathbf{p}] \\ &amp; = \underset{\mathbf{\bigtriangleup p} }{argmin} \sum (\mathbf{e}_{k}^{T}\Omega_{k}\mathbf{e}_{k} +\mathbf{e}_{k}^{T}\Omega_{k}\mathbf{J}_{k}\mathbf{\bigtriangleup p} + \mathbf{\bigtriangleup p}^{T}\mathbf{J}_{k}^{T}\Omega_{k}\mathbf{e}_{k}+\mathbf{\bigtriangleup p}^{T}\mathbf{J}_{k}^{T}\Omega_{k}\mathbf{J}_{k}\mathbf{\bigtriangleup p}) \\ &amp; = \underset{\mathbf{\bigtriangleup p} }{argmin} \sum (\underbrace{\mathbf{e}_{k}^{T}\Omega_{k}\mathbf{e}_{k}}_{常数项} +2\cdot \mathbf{\bigtriangleup p}^{T}\mathbf{J}_{k}^{T}\Omega_{k}\mathbf{e}_{k}+\mathbf{\bigtriangleup p}^{T}\mathbf{J}_{k}^{T}\Omega_{k}\mathbf{J}_{k}\mathbf{\bigtriangleup p}) \\ &amp; = \underset{\mathbf{\bigtriangleup p} }{argmin} \sum (2\cdot \mathbf{\bigtriangleup p}^{T}\mathbf{J}_{k}^{T}\Omega_{k}\mathbf{e}_{k}+\mathbf{\bigtriangleup p}^{T}\mathbf{J}_{k}^{T}\Omega_{k}\mathbf{J}_{k}\mathbf{\bigtriangleup p}) \\ &amp; = \underset{\mathbf{\bigtriangleup p} }{argmin} [2 \mathbf{\bigtriangleup p}^{T}\cdot \sum \mathbf{J}_{k}^{T}\Omega_{k}\mathbf{e}_{k}+\mathbf{\bigtriangleup p}^{T}\cdot \sum \mathbf{J}_{k}^{T}\Omega_{k}\mathbf{J}_{k} \cdot \mathbf{\bigtriangleup p})] \\ \end{align}\] <p>其中，博主为了简化表示，将\(\mathbf{e}_{k}(\mathbf{p})\)简化为\(\mathbf{e}_{k}\)，得到的公式（9）非常重要，它表明，该目标优化问题是一个关于$\Delta\mathbf{p}$的二次型问题！这个发现很重要，决定了优化求解的稳定性。</p> <p>接下来，这里我们计算公式（9）中的目标函数关于\(\Delta \mathbf{p}\)的导数，可以得到</p> \[\begin{matrix}&amp; [2 \mathbf{\bigtriangleup p}^{T}\cdot \sum \mathbf{J}_{k}^{T}\Omega_{k}\mathbf{e}_{k}+\mathbf{\bigtriangleup p}^{T}\cdot \sum \mathbf{J}_{k}^{T}\Omega_{k}\mathbf{J}_{k} \cdot \mathbf{\bigtriangleup p})]^{\prime } \\ \\ &amp; = 2\sum \mathbf{J}_{k}^{T}\Omega_{k}\mathbf{e}_{k} +2\sum \mathbf{J}_{k}^{T}\Omega_{k}\mathbf{J}_{k} \cdot \mathbf{\bigtriangleup p}\end{matrix}\] <p>令其导数\(2\sum \mathbf{J}_{k}^{T}\Omega_{k}\mathbf{e}_{k} +2\sum \mathbf{J}_{k}^{T}\Omega_{k}\mathbf{J}_{k} \cdot \mathbf{\bigtriangleup p} = 0\)，我们可以得到</p> \[2\sum \mathbf{J}_{k}^{T}\Omega_{k}\mathbf{J}_{k} \cdot \mathbf{\bigtriangleup p} + 2\sum \mathbf{J}_{k}^{T}\Omega_{k}\mathbf{e}_{k} = 0\] <p>可能你还觉得不够面熟，我们找个符号代替一下：</p> \[\begin{matrix} &amp; \mathbf{H}\mathbf{\bigtriangleup p}-\mathbf{g} = \mathbf{0} \quad （GN中的增量方程） \\ where,\\ &amp; \mathbf{H} = \sum \mathbf{J}_{k}^{T}\Omega_{k}\mathbf{J}_{k},\\\\ &amp; \mathbf{g} = -\sum \mathbf{J}_{k}^{T}\Omega_{k}\mathbf{e}_{k}\end{matrix}\] <p>这样就熟悉多了，这不就是在线性代数课上学的线性方程求解问题么。没错是的，我们通过化简最终将优化问题转化为一个求解线性方程的问题，怎么求解，这个就不用多说了。这里的\(\mathbf{H}\)，通常被称为海森矩阵（Hessian Matrix），而且海森矩阵\(\mathbf{H}\)要求必须是正定矩阵，上述的化简才是有意义的，为啥嘞？我们能够看到这个线性方程的导数就是\(\mathbf{H}\)（目标函数的二阶导数），我们知道只有当一阶导数为0，二阶导数＞0时（\(\mathbf{H}\)为正定矩阵），我们求出来的才是严格意义上的最小值点。当然这只是一次迭代所求解的变换\(\Delta \mathbf{T}\)，多次迭代收敛后，我们就可以得到完整的刚性变换\(\mathbf{T} = \prod \Delta \mathbf{T}\)。</p> <p>综上所述，给读者朋友们找了（实则网上扒 😏）一个简单的算法流程：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_img/%E4%BB%A5SLAMer%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%85GN%E5%92%8CLM%E7%AE%97%E6%B3%95/GN-480.webp 480w,/assets/img/post_img/%E4%BB%A5SLAMer%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%85GN%E5%92%8CLM%E7%AE%97%E6%B3%95/GN-800.webp 800w,/assets/img/post_img/%E4%BB%A5SLAMer%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%85GN%E5%92%8CLM%E7%AE%97%E6%B3%95/GN-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_img/%E4%BB%A5SLAMer%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%85GN%E5%92%8CLM%E7%AE%97%E6%B3%95/GN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> GN算法的基本流程 </div> <h1 id="3-列文伯格-马夸尔特优化算法levenberg-marquadtlm">3. 列文伯格-马夸尔特优化算法（Levenberg-Marquadt，LM）</h1> <p>我们在第2章逐步推导了GN算法，并且我们得到了<strong>其精髓</strong>：</p> <ol> <li>优化问题的求解——&gt;线性方程组的求解问题</li> <li> <p>海森矩阵的近似：\(\mathbf{H} = \sum \mathbf{J}_{k}^{T}\Omega_{k}\mathbf{J}_{k}\)，即二阶导数可以用一阶导数近似</p> <p>不难看出， GN算法通过在\(\mathbf{p}\)附近进行一阶泰勒展开来简化计算，但这有个前提，那就是\(\Delta \mathbf{p}\)不能太大，否则近似效果很差（因为\(\mathbf{H}\)不一定总是正定的，有可能是半正定的）。用数学的语言来说，\(\Delta \mathbf{p}\)是有一个置信区间的，只有在这个区间里面，我们才能信任它。这就给求解线性方程带来了很大的困扰，本来想着这个变换步长简单易求，但没想到这个步长长度不好把握，步长太短则老太太裹脚，步长太长则小鹿乱撞。</p> <p>于是人们对GN算法作了进一步修正。所用的技巧是把一个绝对正定对角矩阵加到\(\mathbf{H}\)上去 ，改变原矩阵的特征值结构 ，使其变成较好的对称正定矩阵。因此LM中的增量方程为：</p> </li> </ol> \[(\mathbf{H} +\lambda \mathbf{I} )\Delta p = \mathbf{g} \quad （LM中的增量方程）\] <p>其中，\(\lambda\)是一个正实数\(\lambda &gt; 0\)，\(\mathbf{I}\)是一个单位矩阵。哎？这么一操作，发现了一些有趣的事情：</p> <ol> <li><strong>当\(\lambda = 0\) 时</strong>，那LM算法就变成了<strong>GN算法</strong>了</li> <li><strong>当\(\lambda \to \infty\)时</strong>，我们把\(\Delta \mathbf{p}\)显式写出来，可以看到LM算法就变成了<strong>梯度下降法</strong></li> <li><strong>当 \(0&lt; \lambda &lt; \infty\)时，</strong>LM算法的迭代步长<strong>介于GN算法与梯度下降法之间</strong>。</li> </ol> <p>这样我们可以看出LM优化通过在增量方程中增加一个拉格朗日乘子\(\lambda\)来改善GN方法，避免由于\(\Delta \mathbf{p}\)过大，优化解质量不稳定的问题。在一次迭代中， \(\mathbf{H}\)和\(\mathbf{g}\)是不变的，如果我们发觉解出的\(\Delta \mathbf{p}\)过大，就适当调大\(\lambda\)，并重新计算增量方程，以获得相对小一些的\(\Delta \mathbf{p}\)；反过来，如果发觉解出的\(\Delta \mathbf{p}\)在合理的范围内，则适当减小\(\lambda\)，减小后的\(\lambda\)将用于下次迭代，相当于允许下次迭代时\(\Delta \mathbf{p}\)有更大的取值，以尽可能地加快收敛速度。<strong>通过这样的调控，LM算法即保证了收敛的稳定性，又加快了收敛速度。</strong>一切都很nice！</p> <blockquote class="block-tip"> <h4 id="不过还有一个问题我们如何判断delta-mathbfp是否是合理的呢">不过还有一个问题，我们<em>如何判断\(\Delta \mathbf{p}\)是否是合理的呢</em>？</h4> <p>我们可以<em>定义一个总体的loss</em>，来判断\(\Delta \mathbf{p}\)的好坏，对于公式（3）这样一个优化问题来说，一般\(loss = \sum \mathbf{d}_{ij}\)，我们优化的目标当然是让loss最小。如果加上\(\Delta \mathbf{p}\)后loss反而增大了，就证明\(\Delta \mathbf{p}\)超出了置信区间，我们需要调大\(\lambda\)，重新计算增量\(\Delta \mathbf{p}\)；如果loss变小，证明\(\Delta \mathbf{p}\)在合理区间内，调小\(\lambda\)，算法继续。</p> </blockquote> <p>综上所述，LM算法的流程如下:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post_img/%E4%BB%A5SLAMer%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%85GN%E5%92%8CLM%E7%AE%97%E6%B3%95/LM-480.webp 480w,/assets/img/post_img/%E4%BB%A5SLAMer%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%85GN%E5%92%8CLM%E7%AE%97%E6%B3%95/LM-800.webp 800w,/assets/img/post_img/%E4%BB%A5SLAMer%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%85GN%E5%92%8CLM%E7%AE%97%E6%B3%95/LM-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post_img/%E4%BB%A5SLAMer%E8%A7%92%E5%BA%A6%E7%9C%8B%E5%BE%85GN%E5%92%8CLM%E7%AE%97%E6%B3%95/LM.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> LM算法的基本流程 </div> <h1 id="参考资料">参考资料</h1> <p><a href="https://zhuanlan.zhihu.com/p/372136565?utm_id=0">高斯-牛顿优化算法 &amp; L-M优化算法逐行推导</a></p> <p><a href="https://blog.csdn.net/qq_37568167/article/details/105972628">视觉SLAM笔记–第4篇: 高斯牛顿法(GN)和列文伯格-马夸特算法(LM)</a></p> <p><a href="https://blog.csdn.net/qq_42138662/article/details/109289129">高斯牛顿法详解</a></p> <p><a href="https://blog.csdn.net/mppcasc/article/details/119491223">LM优化算法</a></p> <p><a href="https://baike.baidu.com/item/L-M%E6%96%B9%E6%B3%95/2652511">L-M方法-百度百科</a></p> <p>书籍：《视觉slam十四讲》，《最优化理论与算法》第二版</p>]]></content><author><name></name></author><category term="math"/><category term="SLAM,"/><category term="Point-Registration"/><summary type="html"><![CDATA[用白话介绍GN和LM算法]]></summary></entry></feed>